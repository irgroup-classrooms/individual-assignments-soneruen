{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS08 / OR92 Data Modeling: Python - Data Science Packages and Basics of Machine Learning\n",
    "\n",
    "Timo Breuer, Faculty of Information Science and Communication Studies, Institute of Information Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scope of today's session\n",
    "---\n",
    " \n",
    "- Overview of common Python data science packages\n",
    "- Commonly used datasets (in education) \n",
    "- Basic statistics (with pandas)\n",
    "- Data visualisation (with pandas, seaborn, matplotlib)\n",
    "- Basic introduction to machine learning pipelines (with sklearn and tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Python Packages for Data Science \n",
    "--- \n",
    "\n",
    "**Data acquisition / web scraping / parsing**\n",
    "- requests: https://docs.python-requests.org/en/latest/index.html\n",
    "- beautifulsoup: https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "- scrapy: https://scrapy.org/\n",
    "- parsel: https://parsel.readthedocs.io/en/latest/\n",
    "\n",
    "**Data analysis / numerical operations / statistics**\n",
    "- pandas: https://pandas.pydata.org/\n",
    "- numpy: http://www.numpy.org/\n",
    "- scipy: https://www.scipy.org/\n",
    "- statsmodels: https://www.statsmodels.org/stable/index.html\n",
    "\n",
    "**Data visualisation / presentation**\n",
    "- matplotlib: https://matplotlib.org/\n",
    "- seaborn: https://seaborn.pydata.org/\n",
    "- bokeh: https://bokeh.org/\n",
    "- plotly: https://plotly.com/\n",
    "\n",
    "**Machine learning / deep learning** \n",
    "- scikit-learn: https://scikit-learn.org/stable/\n",
    "- tensorflow: https://www.tensorflow.org/\n",
    "- keras: https://keras.io/\n",
    "- pytorch: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Common Datasets\n",
    "---\n",
    "\n",
    "**Note: Many of these datasets can be directly loaded from data science packages, e.g., sklearn or seaborn. There is no need to download the datasets with your own scripts from the original source or somewhere else.**\n",
    "\n",
    "### 1. Iris Dataset\n",
    "\n",
    "**Description:** The Iris dataset is one of the most famous datasets in machine learning. It contains measurements of iris flowers from three different species: Setosa, Versicolor, and Virginica. The dataset includes four features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "**Use Cases:** It is often used for classification tasks, particularly for demonstrating algorithms like k-Nearest Neighbors (k-NN), decision trees, and support vector machines (SVM).\n",
    "\n",
    "**Size:** The dataset consists of 150 samples, with 50 samples for each species.\n",
    "\n",
    "**Source:** https://archive.ics.uci.edu/dataset/53/iris\n",
    "\n",
    "### 2. Titanic Dataset\n",
    "\n",
    "**Description:** The Titanic dataset contains information about the passengers aboard the Titanic, including whether they survived or not. Features include passenger class, sex, age, number of siblings/spouses aboard, number of parents/children aboard, fare paid, and more.\n",
    "\n",
    "**Use Cases:** This dataset is commonly used for binary classification tasks, such as predicting survival based on various features. It is also useful for demonstrating data preprocessing, feature engineering, and exploratory data analysis.\n",
    "\n",
    "**Size:** The dataset contains 891 entries.\n",
    "\n",
    "**Source:** https://www.openml.org/search?type=data&sort=runs&id=40945&status=active\n",
    "\n",
    "### 3. Wine Quality Dataset\n",
    "\n",
    "**Description:** The Wine Quality dataset contains information about red and white wines from Portugal. It includes various physicochemical tests (e.g., acidity, sugar, pH, alcohol content) and a quality score (ranging from 0 to 10) assigned by wine experts. The dataset is divided into two parts: one for red wine and one for white wine.\n",
    "\n",
    "**Use Cases:** This dataset is typically used for regression tasks (predicting the quality score) and classification tasks (classifying wines into quality categories). It is useful for demonstrating regression algorithms, classification algorithms, and techniques for handling imbalanced datasets.\n",
    "\n",
    "**Size:** The red wine dataset contains 1,599 samples, while the white wine dataset contains 4,898 samples.\n",
    "\n",
    "**Source:** https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "### (4. MNIST Dataset)\n",
    "\n",
    "**Description:** The MNIST dataset is a large collection of handwritten digits (0-9) that is widely used for training various image processing systems. It consists of grayscale images of size 28x28 pixels, along with their corresponding labels.\n",
    "\n",
    "**Use Cases:** This dataset is primarily used for image classification tasks and is a standard benchmark for evaluating machine learning algorithms, particularly convolutional neural networks (CNNs).\n",
    "\n",
    "**Size:** The dataset contains 70,000 images, with 60,000 for training and 10,000 for testing.\n",
    "\n",
    "**Source:** https://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "### (5. Boston Housing Dataset)\n",
    "\n",
    "**Description:** The Boston Housing dataset contains information about housing values in suburbs of Boston. It includes features such as the number of rooms, crime rate, property tax rate, and distance to employment centers, among others. The target variable is the median value of owner-occupied homes.\n",
    "\n",
    "**Use Cases:** This dataset is typically used for regression tasks, where the goal is to predict housing prices based on the features. It is often used to demonstrate linear regression and other regression algorithms.\n",
    "\n",
    "**Size:** The dataset consists of 506 samples.\n",
    "\n",
    "**Source:** https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html\n",
    "\n",
    "**WARNING:** This dataset has ethical problems and some packages do not support their use anymore. For instance: https://scikit-learn.org/1.0/modules/generated/sklearn.datasets.load_boston.html This dataset is included in this overview to make you aware of this issue, as this dataset is quite popular.\n",
    "\n",
    "_(In the following, we will focus on the first three datasets. The MNIST dataset is used for image classification tasks and is slightly out of scope. However, it might be useful to know about this dataset at some point in the future. **The use of the Boston Housing Dataset is also not recommended.**)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basic statistics with pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required packages first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy matplotlib seaborn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Descriptive Statistics\n",
    "\n",
    "**Dataset:** Iris  \n",
    "**Problem:** Calculate mean, median, and standard deviation for each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True).frame\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Group-wise Statistics\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Compute average age for passengers grouped by class and survival status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(['class', 'survived'])['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Frequency Distribution\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Count the number of passengers in each embarkation port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['embark_town'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Imbalance Check\n",
    "\n",
    "**Dataset:** Wine Quality  \n",
    "**Problem:** Check the distribution of wine quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Skewness and Kurtosis\n",
    "\n",
    "**Dataset:** Iris  \n",
    "**Problem:** Analyze skewness and kurtosis of petal length and width.\n",
    "\n",
    "**See also:**\n",
    "- https://en.wikipedia.org/wiki/Skewness\n",
    "- https://en.wikipedia.org/wiki/Kurtosis\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(\"Skewness:\", iris[['petal length (cm)', 'petal width (cm)']].apply(skew))\n",
    "print(\"Kurtosis:\", iris[['petal length (cm)', 'petal width (cm)']].apply(kurtosis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Missing Value Analysis\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Check for missing values and calculate the percentage of missing data for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum() / len(titanic) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Feature Comparisons\n",
    "\n",
    "**Dataset:** Wine Quality  \n",
    "**Problem:** Compare average alcohol content for wines of different quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.groupby('quality')['alcohol'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Class Imbalance in a Target Variable\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Assess the survival rate by analyzing the survived column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['survived'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data visualisation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Histogram for Distribution\n",
    "\n",
    "**Dataset:** Wine Quality  \n",
    "**Problem:** Visualize the distribution of alcohol content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "wine['alcohol'].plot(kind='hist', bins=20, title='Alcohol Content Distribution', color='skyblue')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boxplot for Outlier Detection\n",
    "\n",
    "**Dataset:** Iris  \n",
    "**Problem:** Identify outliers in sepal length for different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris(as_frame=True).frame\n",
    "sns.boxplot(x='target', y='sepal length (cm)', data=iris)\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Sepal Length')\n",
    "plt.title('Boxplot of Sepal Length by Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation Heatmap\n",
    "\n",
    "**Dataset:** Wine Quality  \n",
    "**Problem:** Visualize correlation among numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(wine.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Wine Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pairplot for Feature Relationships\n",
    "\n",
    "**Dataset:** Iris  \n",
    "**Problem:** Visualize pairwise relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Create a pairplot\n",
    "sns.set(style='whitegrid')\n",
    "pairplot = sns.pairplot(iris, hue='species', palette='Set2', markers=[\"o\", \"s\", \"D\"])\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle('Pairplot of Iris Dataset', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bar Plot for Counts\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Show the number of survivors and non-survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "sns.countplot(x='survived', data=titanic, palette='pastel')\n",
    "plt.title('Survival Counts')\n",
    "plt.xlabel('Survived (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. FacetGrid for Subplots\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Create age distribution plots for different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(titanic, col='class', height=4, aspect=1.2)\n",
    "g.map(sns.histplot, 'age', bins=20, kde=True, color='purple')\n",
    "g.set_titles('{col_name} Class')\n",
    "g.set_axis_labels('Age', 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Scatter Plot for Feature Relationships\n",
    "\n",
    "**Dataset:** Iris  \n",
    "**Problem:** Visualize the relationship between petal length and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=iris, x='petal_length', y='petal_width', hue='species', palette='deep')\n",
    "plt.title('Petal Length vs. Width by Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Violin Plot for Distribution\n",
    "\n",
    "**Dataset:** Wine Quality  \n",
    "**Problem:** Compare pH levels for different quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='quality', y='pH', data=wine, palette='muted')\n",
    "plt.title('Distribution of pH by Wine Quality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Line Plot for Trends\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Show survival rate trends by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_rate_by_age = titanic.groupby('age')['survived'].mean().dropna()\n",
    "survival_rate_by_age.plot(kind='line', figsize=(10, 5), color='green')\n",
    "plt.title('Survival Rate by Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Stacked Bar Chart for Group Comparisons\n",
    "\n",
    "**Dataset:** Titanic  \n",
    "**Problem:** Compare survival rates across gender and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_class_gender = titanic.pivot_table(index='class', columns='sex', values='survived', aggfunc='mean')\n",
    "survival_class_gender.plot(kind='bar', stacked=True, figsize=(8, 6), color=['orange', 'lightblue'])\n",
    "plt.title('Survival Rate by Class and Gender')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basic introduction to machine learning pipelines\n",
    "---\n",
    "\n",
    "There are many machine learning methods and we cannot cover all of them in today's session. To get an overview of relevant and related topics, take a look at scikit-learn that covers many ML methods and concepts: https://scikit-learn.org/stable/index.html\n",
    "\n",
    "Important topics are among others:\n",
    "- Classification Methods\n",
    "- Regression Methods\n",
    "- Clustering Methods\n",
    "- Dimensionality Reduction\n",
    "- Preprocessing Methods\n",
    "- Model Selection and Evaluation\n",
    "\n",
    "Instead of going into breadth, we take a look at some popular methods, including **k-nearest neighbors**, **logistic regression**, **random forest**, and **neural networks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors is a simple, instance-based learning algorithm used for classification and regression. The algorithm classifies a data point based on how its neighbors are classified. \n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. Choose the number of neighbors $ k $.\n",
    "2. For a given test point, calculate the distance to all training points.\n",
    "3. Identify the $ k $ nearest neighbors.\n",
    "4. Assign the class label based on the majority class among the neighbors.\n",
    "\n",
    "**Distance Metric:**\n",
    "The most common distance metric used is the Euclidean distance, defined as:\n",
    "\n",
    "$$d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$$\n",
    "\n",
    "where $ p $ and $ q $ are two points in $ n $-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method for predicting binary classes. The output of the model is a probability that the given input point belongs to a certain class.\n",
    "\n",
    "**Model Equation:**\n",
    "The logistic function (sigmoid function) is used to map predicted values to probabilities:\n",
    "\n",
    "$$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n)}}$$\n",
    "\n",
    "where:\n",
    "- $ P(Y=1|X) $ is the probability of the positive class given input features $ X $.\n",
    "- $ \\beta_0 $ is the intercept.\n",
    "- $ \\beta_1, \\beta_2, \\ldots, \\beta_n $ are the coefficients for each feature $ X_1, X_2, \\ldots, X_n $.\n",
    "\n",
    "**Decision Boundary:**\n",
    "The decision boundary is determined by setting the probability threshold (commonly 0.5):\n",
    "\n",
    "$$\\text{If } P(Y=1|X) \\geq 0.5 \\text{, predict } Y=1; \\text{ otherwise, predict } Y=0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic') \n",
    "\n",
    "# Preprocess the data (basic preprocessing)\n",
    "titanic_df['sex'] = titanic_df['sex'].map({'male': 0, 'female': 1})\n",
    "titanic_df = titanic_df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'survived']].dropna()\n",
    "\n",
    "# Split the dataset\n",
    "X = titanic_df.drop('survived', axis=1)\n",
    "y = titanic_df['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees.\n",
    "\n",
    "**Model Concept:**\n",
    "1. **Bootstrap Aggregating (Bagging):** Randomly sample the dataset with replacement to create multiple subsets.\n",
    "2. **Decision Trees:** For each subset, build a decision tree using a random subset of features.\n",
    "3. **Voting/Averaging:** For classification, the final prediction is made by majority voting among the trees.\n",
    "\n",
    "**Decision Tree Splitting Criterion:**\n",
    "The most common criteria for splitting nodes in a decision tree are Gini impurity and entropy.\n",
    "\n",
    "- **Gini Impurity:**\n",
    "$$Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "where $ p_i $ is the proportion of class $ i $ in dataset $ D $.\n",
    "\n",
    "- **Entropy:**\n",
    "$$Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "where $ C $ is the number of classes.\n",
    "\n",
    "**Final Prediction:**\n",
    "For classification tasks, the final prediction is made by taking the majority vote from all the individual decision trees in the forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine Quality dataset\n",
    "# wine_df = pd.read_csv('winequality-red.csv')  # Ensure you have the dataset in the same directory\n",
    "wine_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "\n",
    "# Split the dataset\n",
    "X = wine_df.drop('quality', axis=1)\n",
    "y = wine_df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Iris Dataset with TensorFlow\n",
    "\n",
    "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of layers of interconnected nodes (neurons) that can learn complex patterns in data.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input Layer: 4 neurons (one for each feature: sepal length, sepal width, petal length, petal width).\n",
    "- Hidden Layer: A configurable number of neurons (e.g., 16).\n",
    "- Hidden Layer: A configurable number of neurons (e.g., 8).\n",
    "- Output Layer: 3 neurons (one for each class of Iris).\n",
    "\n",
    "**Activation Function:**\n",
    "The activation function introduces non-linearity into the model. A common choice is the ReLU (Rectified Linear Unit) function:\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "For the output layer, the softmax function is often used for multi-class classification:\n",
    "\n",
    "$$P(y_i|x) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}$$\n",
    "\n",
    "**Loss Function:**\n",
    "For multi-class classification, the categorical cross-entropy loss is commonly used:\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "where $ y $ is the true label and $ \\hat{y} $ is the predicted probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target.reshape(-1, 1)  # Target (reshaped to column vector)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the Neural Network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer 1\n",
    "    tf.keras.layers.Dense(8, activation='relu'),  # Hidden layer 2\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # Output layer (3 classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict on new data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Display predictions and true classes\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", true_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What happened in the code above?_ \n",
    "\n",
    "**Neural Network Structure:**  \n",
    "- **Input Layer:** Takes 4 features (sepal length, sepal width, petal length, petal width).\n",
    "- **Hidden Layers:** Two hidden layers with ReLU activation.\n",
    "- **Output Layer:** Three neurons (for the three Iris classes) with softmax activation.\n",
    "Preprocessing: One-Hot Encoding: Converts class labels (0, 1, 2) into one-hot vectors. Feature Scaling: Standardizes the feature values for better performance.\n",
    "\n",
    "**Loss Function:** categorical_crossentropy is used since this is a multi-class classification problem.\n",
    "\n",
    "**Metrics:** Accuracy is used as the performance metric.\n",
    "\n",
    "**Training:** Uses the Adam optimizer with 100 epochs and a batch size of 8. A validation split of 20% is used for monitoring overfitting.\n",
    "\n",
    "**Evaluation:** The model is evaluated on the test set, and accuracy is reported. Predictions are converted from probabilities to class labels using np.argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Titanic Dataset with TensorFlow\n",
    "\n",
    "A neural network can be used to predict survival on the Titanic based on various features such as passenger class, sex, age, and fare.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input Layer: Number of neurons equal to the number of features (e.g., 6 after preprocessing).\n",
    "- Hidden Layer: A configurable number of neurons (e.g., 10-20).\n",
    "- Output Layer: 1 neuron (for binary classification: survived or not).\n",
    "\n",
    "**Activation Function:**\n",
    "Commonly used activation functions include ReLU for hidden layers:\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "For the output layer, the sigmoid function is used for binary classification:\n",
    "\n",
    "$$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n)}}$$\n",
    "\n",
    "**Loss Function:**\n",
    "For binary classification, the binary cross-entropy loss is used:\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_df = sns.load_dataset('titanic') \n",
    "\n",
    "# Preprocess the data (basic preprocessing)\n",
    "titanic_df['sex'] = titanic_df['sex'].map({'male': 0, 'female': 1})\n",
    "titanic_df = titanic_df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'survived']].dropna()\n",
    "\n",
    "# Split the dataset\n",
    "X = titanic_df.drop('survived', axis=1)\n",
    "y = titanic_df['survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What happened in the code above?_ \n",
    " \n",
    "**Data Loading and Preprocessing:** The Titanic dataset is loaded, and basic preprocessing is performed, including encoding the 'sex' feature and dropping rows with missing values.\n",
    "\n",
    "**Feature and Target Separation:** The features (X) and target variable (y) are separated.\n",
    "\n",
    "**Train-Test Split:** The dataset is split into training and testing sets.\n",
    "\n",
    "**Feature Scaling:** The features are standardized using StandardScaler.\n",
    "\n",
    "**Model Building:** A simple neural network model is created with one hidden layer using ReLU activation and an output layer with a sigmoid activation function.\n",
    "\n",
    "**Model Compilation:** The model is compiled with the Adam optimizer and binary cross-entropy loss.\n",
    "\n",
    "**Model Training:** The model is trained on the training data.\n",
    "\n",
    "**Model Evaluation:** The model is evaluated on the test data, and the accuracy is printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for Wine Quality Dataset with TensorFlow\n",
    "\n",
    "A neural network can be used to predict the quality of wine based on various chemical properties.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input Layer: Number of neurons equal to the number of features (e.g., 11 for the Wine Quality dataset).\n",
    "- Hidden Layer: A configurable number of neurons (e.g., 10-30).\n",
    "- Output Layer: 1 neuron (for regression predicting wine quality on a scale).\n",
    "\n",
    "**Activation Function:**\n",
    "ReLU is commonly used for hidden layers:\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "For the output layer, a linear activation function is used for regression:\n",
    "\n",
    "$$f(x) = x$$\n",
    "\n",
    "**Loss Function:**\n",
    "For regression tasks, the mean squared error (MSE) is commonly used:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the Wine Quality dataset\n",
    "# wine_df = pd.read_csv('winequality-red.csv')  # Ensure you have the dataset in the same directory\n",
    "wine_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = wine_df.drop('quality', axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=5, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f'Mean Absolute Error: {mae:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What happened in the code above?_ \n",
    "\n",
    "**Data Loading:** The Wine Quality dataset is loaded into a DataFrame.\n",
    "\n",
    "**Feature and Target Separation:** The features (X) and target variable (y) are separated, with 'quality' as the target.\n",
    "\n",
    "**Train-Test Split:** The dataset is split into training and testing sets.\n",
    "\n",
    "**Feature Scaling:** The features are standardized using StandardScaler.\n",
    "\n",
    "**Model Building:** A simple neural network model is created with one hidden layer using ReLU activation and an output layer with a linear activation function.\n",
    "\n",
    "**Model Compilation:** The model is compiled with the Adam optimizer and mean squared error loss.\n",
    "\n",
    "**Model Training:** The model is trained on the training data.\n",
    "\n",
    "**Model Evaluation:** The model is evaluated on the test data, and the mean absolute error (MAE) is printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab assignment\n",
    "---\n",
    "\n",
    "In order to recap the things you have learned today. You will apply the concepts and methods to another dataset. **Pick one** of the following datasets and conduct some reasonable methods for basic statistics, data visualization, and machine learning.\n",
    "\n",
    "- **Adult Income Dataset:** https://archive.ics.uci.edu/ml/datasets/adult\n",
    "- **Heart Disease Dataset:** https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "- **NYC Airbnb Listings:** http://insideairbnb.com/get-the-data.html\n",
    "- **COVID-19 Data:** https://github.com/owid/covid-19-data\n",
    "- **Car Evaluation Dataset:** https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
    "- **World Happiness Report:** https://worldhappiness.report/\n",
    "- **Supermarket Sales Dataset:** https://www.kaggle.com/aungpyaeap/supermarket-sales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
